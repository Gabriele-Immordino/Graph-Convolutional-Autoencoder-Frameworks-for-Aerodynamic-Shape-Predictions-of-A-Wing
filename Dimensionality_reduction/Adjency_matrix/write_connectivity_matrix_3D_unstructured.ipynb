{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Connectivity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mesh_reader(file, coordinates,marker_elements, marker):\n",
    "    # This method is for reading mesh files\n",
    "    # indicated for 3D su2 files\n",
    "    nelements = 0\n",
    "    markers_id = []\n",
    "    \n",
    "    with open(coordinates, 'w') as fout, open(marker_elements, 'w') as fout2, open(file, 'r') as fin:\n",
    "        while True:\n",
    "            line = fin.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            \n",
    "            if 'NPOIN=' in line:\n",
    "                _, n_poin_str = line.split('=')\n",
    "                n_poin = int(n_poin_str)\n",
    "                print(f'total number of points {n_poin}')\n",
    "                counter = 0\n",
    "                while counter < n_poin:\n",
    "                    line = fin.readline()\n",
    "                    line_ = line.split()\n",
    "                    fout.write(f'{line_[0]},{line_[1]},{line_[2]},{line_[3]}\\n')\n",
    "                    counter += 1\n",
    "\n",
    "            \n",
    "            if f'MARKER_TAG= {marker}' in line:\n",
    "                print(line)\n",
    "                line = fin.readline()\n",
    "                line = line.split()\n",
    "                nelements = int(line[1])\n",
    "                print(f'number of marker tag elements {nelements}')\n",
    "                for _ in range(nelements):\n",
    "                    line = fin.readline()\n",
    "                    line_ = line.split()\n",
    "                    fout2.write(f'{line_[1]},{line_[2]},{line_[3]}\\n')\n",
    "                    # markers_id.append(int(line[1]))\n",
    "                break\n",
    "    \n",
    "    fout.close()          \n",
    "    fout2.close()\n",
    "\n",
    "    coor = []\n",
    "    mark_elem = []\n",
    "    print('importing coordinates of whole model')\n",
    "    \n",
    "    with open(coordinates, 'r') as coordinates:\n",
    "        for line in coordinates:\n",
    "            row = line.split(',')\n",
    "            coor.append([float(row[0]), float(row[1]),float(row[2]), int(row[3])])\n",
    "    \n",
    "    with open(marker_elements, 'r') as marker_elements:\n",
    "        for line in marker_elements:\n",
    "            row = line.split(',')\n",
    "            mark_elem.append([int(row[0]), int(row[1]), int(row[2])])\n",
    "    \n",
    "\n",
    "    print('Mesh reader: process terminated')\n",
    "    return coor, mark_elem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_file = 'Agard_446p5_v3.su2'\n",
    "coordinates = 'coordinates.csv'\n",
    "marker_elements = 'marker_elements.csv'\n",
    "\n",
    "MARKERS = {'wing'}\n",
    "for item in MARKERS:\n",
    "    coor_, mark_elem_ = mesh_reader(mesh_file, coordinates,marker_elements, item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "coordinates = pd.read_csv('coordinates.csv', header=None)\n",
    "surface = pd.read_csv('surface.csv')\n",
    "coord_surface = surface['PointID']\n",
    "\n",
    "# Extract the last column from coordinates and filter it\n",
    "last_column = coordinates.iloc[:, -1]  # Assuming the last column is the one you want\n",
    "filtered_coordinates = coordinates[last_column.isin(coord_surface)]\n",
    "point_id_reoder = list(range(len(filtered_coordinates.iloc[:, -1])))\n",
    "filtered_coordinates.iloc[:, -1] = point_id_reoder\n",
    "filtered_coordinates.to_csv('coordinates.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder the marker elements\n",
    "import csv\n",
    "\n",
    "# Read the CSV file\n",
    "filename = 'marker_elements.csv'\n",
    "data = []\n",
    "with open(filename, 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        data.append(row)\n",
    "\n",
    "# Create a mapping between original numbers and new numbers\n",
    "unique_numbers = set()\n",
    "for row in data:\n",
    "    unique_numbers.update(map(int, row))\n",
    "number_mapping = {number: idx for idx, number in enumerate(sorted(unique_numbers))}\n",
    "\n",
    "# Read the CSV file again and replace numbers with new values\n",
    "data = []\n",
    "with open(filename, 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        new_row = [number_mapping[int(number)] for number in row]  # Convert each value to int\n",
    "        data.append(new_row)\n",
    "\n",
    "# Save the modified data to a new CSV file\n",
    "modified_filename = 'marker_elements.csv'\n",
    "with open(modified_filename, 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Connectivity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Read the file containing point connections with three columns\n",
    "with open('marker_elements.csv', 'r') as conn_file:\n",
    "    connections = [line.strip().split(',') for line in conn_file]\n",
    "\n",
    "\n",
    "# Create a list to store normalized adjacency entries\n",
    "adjacency_entries_1 = []\n",
    "adjacency_entries_2 = []\n",
    "adjacency_entries_3 = []\n",
    "\n",
    "\n",
    "for conn in connections:\n",
    "    point1, point2, point3 = int(conn[0]), int(conn[1]), int(conn[2])\n",
    "    \n",
    "    # Add the entries to the list\n",
    "    adjacency_entries_1.append([point1, point2])\n",
    "    adjacency_entries_2.append([point2, point3])\n",
    "    adjacency_entries_3.append([point3, point1])\n",
    "\n",
    "# Convert the lists to NumPy arrays\n",
    "adjacency_array_1 = np.array(adjacency_entries_1)\n",
    "adjacency_array_2 = np.array(adjacency_entries_2)\n",
    "adjacency_array_3 = np.array(adjacency_entries_3)\n",
    "\n",
    "# Combine the arrays vertically\n",
    "adjacency_array = np.vstack((adjacency_array_1, adjacency_array_2, adjacency_array_3))\n",
    "\n",
    "# Save the NumPy array to a CSV file\n",
    "np.savetxt('connectivity_matrix.csv', adjacency_array, fmt='%d', delimiter=',', header='point_i,point_j', comments='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Edge_weight matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "\n",
    "dataset_coords = np.load('Dataset\\\\agard_doe_r002_dataset_reduced.npy')\n",
    "dataset_coords = dataset_coords[:,:,:3]\n",
    "\n",
    "edge_weight = []\n",
    "coordinates = pd.read_csv('coordinates.csv', header=None)\n",
    "point_id = np.array(coordinates[3])\n",
    "\n",
    "# Read the file containing point connections with three columns\n",
    "with open('marker_elements.csv', 'r') as conn_file:\n",
    "    connections = [line.strip().split(',') for line in conn_file]\n",
    "\n",
    "for sample in range(dataset_coords.shape[0]):\n",
    "    print(sample)\n",
    "    coordinate_dict = {point_id[id]: (float(dataset_coords[sample,id,0]), float(dataset_coords[sample,id,1]), float(dataset_coords[sample,id,2])) for id in range(point_id.shape[0])}\n",
    "\n",
    "    # Create a list to store normalized adjacency entries\n",
    "    adjacency_entries_1 = []\n",
    "    adjacency_entries_2 = []\n",
    "    adjacency_entries_3 = []\n",
    "\n",
    "    # Find the maximum distances for normalization\n",
    "    max_distance = float('-inf')\n",
    "\n",
    "    # Populate the list with entries (point1, point2, point3, normalized_distance)\n",
    "    for conn in connections:\n",
    "        point1, point2, point3 = int(conn[0]), int(conn[1]), int(conn[2])\n",
    "        coord1, coord2, coord3 = coordinate_dict[point1], coordinate_dict[point2], coordinate_dict[point3]\n",
    "        \n",
    "        # Calculate the distances between the points\n",
    "        dist1 = distance.euclidean(coord1, coord2)\n",
    "        dist2 = distance.euclidean(coord2, coord3)\n",
    "        dist3 = distance.euclidean(coord3, coord1)\n",
    "        \n",
    "        # Take the inverse of the distances\n",
    "        inv_dist1 = 1.0 / dist1\n",
    "        inv_dist2 = 1.0 / dist2\n",
    "        inv_dist3 = 1.0 / dist3\n",
    "        \n",
    "        # Update max distances\n",
    "        max_distance = max(max_distance, inv_dist1, inv_dist2, inv_dist3)\n",
    "\n",
    "    # Normalize and add entries to the list\n",
    "    for conn in connections:\n",
    "        point1, point2, point3 = int(conn[0]), int(conn[1]), int(conn[2])\n",
    "        coord1, coord2, coord3 = coordinate_dict[point1], coordinate_dict[point2], coordinate_dict[point3]\n",
    "        \n",
    "        # Calculate the distances between the points\n",
    "        dist1 = distance.euclidean(coord1, coord2)\n",
    "        dist2 = distance.euclidean(coord2, coord3)\n",
    "        dist3 = distance.euclidean(coord3, coord1)\n",
    "\n",
    "        # Take the inverse of the distances\n",
    "        inv_dist1 = 1.0 / dist1\n",
    "        inv_dist2 = 1.0 / dist2\n",
    "        inv_dist3 = 1.0 / dist3\n",
    "        \n",
    "        # Normalize the inverse distances between 0 and 1\n",
    "        normalized_distance1 = inv_dist1 / max_distance\n",
    "        normalized_distance2 = inv_dist2 / max_distance\n",
    "        normalized_distance3 = inv_dist3 / max_distance\n",
    "        \n",
    "        # Add the entries to the list\n",
    "        adjacency_entries_1.append([point1, point2, normalized_distance1])\n",
    "        adjacency_entries_2.append([point2, point3, normalized_distance2])\n",
    "        adjacency_entries_3.append([point3, point1, normalized_distance3])\n",
    "\n",
    "\n",
    "    # Convert the lists to NumPy arrays\n",
    "    adjacency_array_1 = np.array(adjacency_entries_1)\n",
    "    adjacency_array_2 = np.array(adjacency_entries_2)\n",
    "    adjacency_array_3 = np.array(adjacency_entries_3)\n",
    "\n",
    "    # Combine the arrays vertically\n",
    "    adjacency_array = np.vstack((adjacency_array_1, adjacency_array_2, adjacency_array_3))\n",
    "\n",
    "    edge_weight.append(adjacency_array[:,-1])\n",
    "\n",
    "np.save('edge_weight_array.npy',np.array(edge_weight))\n",
    "print(np.array(edge_weight).shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
