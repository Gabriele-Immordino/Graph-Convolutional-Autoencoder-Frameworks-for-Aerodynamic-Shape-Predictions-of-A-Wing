{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch_geometric\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from functions.space import Space\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import optuna \n",
    "from indices_selection_splitting import split_dataset_stratified\n",
    "\n",
    "seed_value = 42\n",
    "torch.manual_seed(seed_value)\n",
    "torch.cuda.manual_seed(seed_value)\n",
    "torch.cuda.manual_seed_all(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "random.seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run first the script 'Dimensionality_reduction\\dimensionality_reduction_module.ipynb' to generate the encoding and decoding of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"PyTorch Geometric version: {torch_geometric.__version__}\")\n",
    "def create_path(name):\n",
    "    if not os.path.exists(name):\n",
    "        os.makedirs(name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Study and storage settings\n",
    "study_name = 'autoencoder_GCN_optimization_1'\n",
    "storage_url = f'sqlite:///{study_name}.db'\n",
    "\n",
    "test_name = 'Opt_GCN_v1'\n",
    "\n",
    "dim_red_dir = os.path.join('Dimensionality_reduction', 'output')\n",
    "dataset_dir = os.path.join('Dataset', 'agard_dataset.npy')\n",
    "grid_data_dir = os.path.join('Dataset', 'grid_data')\n",
    "\n",
    "output_dir = os.path.join('output', test_name)\n",
    "model_dir = os.path.join('Model_architecture', test_name)\n",
    "model_filename = \"best_model.pth\"\n",
    "\n",
    "create_path(output_dir)\n",
    "create_path(model_dir)\n",
    "\n",
    "\n",
    "\n",
    "# Transfer Learning: True-> loads the stored weights and trains from there; False-> trains from new\n",
    "Transfer_Learning_flag = True\n",
    "# Training Flag: True-> model.fit is executed and saved; False-> not trained neither stored\n",
    "Execute_Train_flag = True\n",
    "\n",
    "#############################\n",
    "# Wing geometry specs\n",
    "xref = 0.4637/2    # Reference point for the x-coordinate (half of the chord length)\n",
    "chord = 0.4637     # Chord length of the wing\n",
    "span = 0.75        # Span of the wing\n",
    "area = 0.353       # Reference area of the wing\n",
    "#############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = np.load(dataset_dir) # Shape: (251, 45943, 13) - (samples, nodes, features) # features: [x, y, z, CP, CFx, CFy, CFz, mode1, mode2, mode3, mode4, mode5, mode6]\n",
    "\n",
    "X = dataset[:, :, np.concatenate((np.arange(3), np.arange(7, dataset.shape[2])))]\n",
    "coordinates = X[:,:,:3]\n",
    "Y = dataset[:,:,3:7] # Output\n",
    "print( '-> Shape of the input matrix: ',X.shape)\n",
    "print( '-> Shape of the output matrix: ',Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature normalisation:\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "samples, points, variables = X.shape\n",
    "X = np.reshape(X, newshape=(-1, variables))\n",
    "X = scaler.fit_transform(X)\n",
    "X = np.reshape(X, newshape=(samples, points, variables))\n",
    "\n",
    "scalery = MinMaxScaler(feature_range=(-1, 1))\n",
    "samples, points, variables = Y.shape\n",
    "Y = np.reshape(Y, newshape=(-1, variables))\n",
    "Y = scalery.fit_transform(Y)\n",
    "Y = np.reshape(Y, newshape=(samples, points, variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices, val_indices, test_indices = split_dataset_stratified(X, Y, \n",
    "                                                                    k=20,\n",
    "                                                                    train_ratio=0.65,\n",
    "                                                                    val_ratio=0.25,\n",
    "                                                                    test_ratio=0.15,\n",
    "                                                                    )  # Split the dataset into train, val, and test indices using stratified sampling\n",
    "\n",
    "X_train, X_val, X_test, Y_train, Y_val, Y_test = X[train_indices], X[val_indices], X[test_indices], Y[train_indices], Y[val_indices], Y[test_indices]  # Create train, val, and test splits for X and Y\n",
    "\n",
    "# Create the dictionary\n",
    "indices_dict = {\n",
    "    'train_indices': train_indices,  # Store train indices\n",
    "    'val_indices': val_indices,      # Store validation indices\n",
    "    'test_indices': test_indices     # Store test indices\n",
    "}\n",
    "\n",
    "print(f\"Train len: {len(train_indices)}\")  \n",
    "print(f\"Val len: {len(val_indices)}\")      \n",
    "print(f\"Test len: {len(test_indices)}\")    \n",
    "print(f\"Tot len: {len(train_indices)+len(val_indices)+len(test_indices)}\")  \n",
    "\n",
    "# Save the dictionary to a pickle file\n",
    "with open('indices.pkl', 'wb') as f:\n",
    "    pickle.dump(indices_dict, f)  # Save indices dictionary to a pickle file\n",
    "\n",
    "print(\"Indices saved to indices.pkl\")  # Confirm that indices have been saved\n",
    "\n",
    "np.random.seed(seed_value)  # Set numpy random seed for reproducibility\n",
    "random.seed(seed_value)     # Set python random seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the code for loading the variables:\n",
    "\n",
    "space_data = []\n",
    "\n",
    "for sample in range(dataset.shape[0]):\n",
    "    print(f\"{sample+1}/{dataset.shape[0]}\", end=\"\\r\")  # Print progress for each sample\n",
    "    output_dim_red_dir = dim_red_dir + f'Sample_{sample:03d}\\\\'  # Build path to the sample's directory\n",
    "    sp1_path = output_dim_red_dir + \"space1\"  # Path to space1 file\n",
    "    sp2_path = output_dim_red_dir + \"space2\"  # Path to space2 file\n",
    "    space1 = Space()  # Create Space object for space1\n",
    "    space2 = Space()  # Create Space object for space2\n",
    "    space1.load(sp1_path)  # Load data into space1 from file\n",
    "    space2.load(sp2_path)  # Load data into space2 from file\n",
    "    data = (space1, space2)  # Tuple containing both space objects\n",
    "    space_data.append(data)  # Append tuple to space_data list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_data = []\n",
    "\n",
    "for i in range(dataset.shape[0]):  \n",
    "    grid_data_file = torch.tensor(np.load(os.path.join(grid_data_dir, f'grid_data_{i:03d}.npy')), dtype=torch.float32).to(device)  # Load grid data for sample i and move to device\n",
    "    cell_area, X_Grid_K_Unit_Normal, Y_Grid_K_Unit_Normal, Z_Grid_K_Unit_Normal = grid_data_file[:,0], grid_data_file[:,1], grid_data_file[:,2], grid_data_file[:,3]  # Extract relevant columns\n",
    "    coordinates_tensor = torch.tensor(coordinates[i,:,:], dtype=torch.float32).to(device)  # Convert coordinates for sample i to tensor and move to device\n",
    "    grid_data.append((cell_area, X_Grid_K_Unit_Normal, Y_Grid_K_Unit_Normal, Z_Grid_K_Unit_Normal, coordinates_tensor))  # Append tuple to grid_data list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom PyTorch Geometric Dataset\n",
    "\n",
    "class CustomGraphData(Data):\n",
    "    def __init__(self, x, y, space, grid_data):\n",
    "        \"\"\"\n",
    "        Custom data object for graph data, extending torch_geometric.data.Data.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Node features.\n",
    "            y (Tensor): Target values.\n",
    "            space (tuple): Tuple of Space objects for graph structure.\n",
    "            grid_data (tuple): Tuple containing grid-related tensors.\n",
    "        \"\"\"\n",
    "        super().__init__(x, y)\n",
    "        self.x = x  # Node features\n",
    "        self.y = y  # Target values\n",
    "        self.space = space  # Space objects for graph structure\n",
    "        self.grid_data = grid_data  # Grid-related tensors\n",
    "\n",
    "class CustomGraphDataset(Dataset):\n",
    "    def __init__(self, X, Y, space, grid_data):\n",
    "        \"\"\"\n",
    "        Custom dataset for graph data.\n",
    "\n",
    "        Args:\n",
    "            X (np.ndarray): Input features.\n",
    "            Y (np.ndarray): Target values.\n",
    "            space (list): List of Space tuples.\n",
    "            grid_data (list): List of grid data tuples.\n",
    "        \"\"\"\n",
    "        if not (len(X) == len(Y) and len(Y) == len(space)):\n",
    "            raise ValueError(\"X Y and space must be the same length\")  # Ensure all inputs have the same length\n",
    "        self.X = X  # Input features\n",
    "        self.Y = Y  # Target values\n",
    "        self.space = space  # Space objects\n",
    "        self.grid_data = grid_data  # Grid data\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of samples in the dataset.\"\"\"\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get a single sample from the dataset.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the sample.\n",
    "\n",
    "        Returns:\n",
    "            CustomGraphData: Data object containing features, targets, space, and grid data.\n",
    "        \"\"\"\n",
    "        x = torch.tensor(self.X[idx], dtype=torch.float32)  # Convert input features to tensor\n",
    "        y = torch.tensor(self.Y[idx], dtype=torch.float32)  # Convert target values to tensor\n",
    "        space = self.space[idx]  # Get space tuple\n",
    "        grid_data = self.grid_data[idx]  # Get grid data tuple\n",
    "        data = CustomGraphData(x=x, y=y, space=space, grid_data=grid_data)  # Create data object\n",
    "        return data\n",
    "\n",
    "# Create instances of the custom Dataset for train, val, and test splits\n",
    "space_data_train = [space_data[i] for i in train_indices]  # Training space data\n",
    "space_data_val = [space_data[i] for i in val_indices]      # Validation space data\n",
    "space_data_test = [space_data[i] for i in test_indices]    # Test space data\n",
    "\n",
    "grid_data_train = [grid_data[i] for i in train_indices]    # Training grid data\n",
    "grid_data_val = [grid_data[i] for i in val_indices]        # Validation grid data\n",
    "grid_data_test = [grid_data[i] for i in test_indices]      # Test grid data\n",
    "\n",
    "train_dataset = CustomGraphDataset(X_train, Y_train, space_data_train, grid_data_train)  # Training dataset\n",
    "val_dataset = CustomGraphDataset(X_val, Y_val, space_data_val, grid_data_val)           # Validation dataset\n",
    "test_dataset = CustomGraphDataset(X_test, Y_test, space_data_test, grid_data_test)       # Test dataset\n",
    "\n",
    "def collate(data_list):\n",
    "    \"\"\"\n",
    "    Custom collate function to move data to the appropriate device.\n",
    "\n",
    "    Args:\n",
    "        data_list (list): List of CustomGraphData objects.\n",
    "\n",
    "    Returns:\n",
    "        list: List of data objects moved to device if CUDA is available.\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        data_list = [data.to(device) for data in data_list]  # Move each data object to device\n",
    "    return data_list\n",
    "\n",
    "batch_size = 1  # Set batch size\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    \"\"\"\n",
    "    Seed worker function for reproducibility in DataLoader workers.\n",
    "\n",
    "    Args:\n",
    "        worker_id (int): Worker ID.\n",
    "    \"\"\"\n",
    "    worker_seed = torch.initial_seed() % 2**32  # Generate worker seed\n",
    "    np.random.seed(worker_seed)  # Seed numpy\n",
    "    random.seed(worker_seed)     # Seed random\n",
    "\n",
    "g = torch.Generator()  # Create a torch generator\n",
    "g.manual_seed(seed_value)  # Set manual seed for reproducibility\n",
    "\n",
    "# Create DataLoaders for train, validation, and test sets\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True,\n",
    "    worker_init_fn=seed_worker, generator=g, collate_fn=collate\n",
    ")  # Training DataLoader\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=True,\n",
    "    worker_init_fn=seed_worker, generator=g, collate_fn=collate\n",
    ")  # Validation DataLoader\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=batch_size, shuffle=False,\n",
    "    worker_init_fn=seed_worker, generator=g, collate_fn=collate\n",
    ")  # Test DataLoader\n",
    "\n",
    "total_samples = len(train_loader.dataset)  # Total samples in training set\n",
    "batch_size = train_loader.batch_size       # Batch size used\n",
    "num_batches = len(train_loader)            # Number of batches in training set\n",
    "\n",
    "print(f\"Total number of samples in train_loader: {total_samples}\")  # Print total samples\n",
    "print(f\"Batch size in train_loader: {batch_size}\")                  # Print batch size\n",
    "print(f\"Number of batches in train_loader: {num_batches}\")          # Print number of batches\n",
    "\n",
    "batch = next(iter(train_loader))  # Get the first batch\n",
    "print(\"Batch:\", batch)            # Print the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConv(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Graph Convolutional Layer using GCNConv with optional output activation.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, output=False):\n",
    "        super(GraphConv, self).__init__()\n",
    "        self.conv = GCNConv(in_channels, out_channels, add_self_loops=True, improved=True, cached=True)  # GCN layer\n",
    "        self.prelu = nn.PReLU()  # PReLU activation\n",
    "        self.output = output  # Whether this is an output layer\n",
    "\n",
    "    def forward(self, x, edge_indices, edge_distances):\n",
    "        \"\"\"\n",
    "        Forward pass for GraphConv.\n",
    "        Args:\n",
    "            x: Node features.\n",
    "            edge_indices: Edge indices.\n",
    "            edge_distances: Edge weights.\n",
    "        Returns:\n",
    "            Output tensor after convolution and activation.\n",
    "        \"\"\"\n",
    "        x = self.conv(x, edge_index=edge_indices, edge_weight=edge_distances)  # Apply GCN\n",
    "        if not self.output:\n",
    "            x = self.prelu(x)  # Apply activation if not output\n",
    "        return x\n",
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder module using sparse matrix multiplication.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "    def forward(self, x, encoder_interpolation_matrix):\n",
    "        \"\"\"\n",
    "        Forward pass for Encoder.\n",
    "        Args:\n",
    "            x: Input features.\n",
    "            encoder_interpolation_matrix: Sparse interpolation matrix.\n",
    "        Returns:\n",
    "            Encoded features.\n",
    "        \"\"\"\n",
    "        return torch.sparse.mm(encoder_interpolation_matrix, x)  # Sparse matrix multiplication\n",
    "\n",
    "class Decoder(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder module using sparse matrix multiplication.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "    def forward(self, x, decoder_interpolation_matrix):\n",
    "        \"\"\"\n",
    "        Forward pass for Decoder.\n",
    "        Args:\n",
    "            x: Input features.\n",
    "            decoder_interpolation_matrix: Sparse interpolation matrix.\n",
    "        Returns:\n",
    "            Decoded features.\n",
    "        \"\"\"\n",
    "        return torch.sparse.mm(decoder_interpolation_matrix, x)  # Sparse matrix multiplication\n",
    "\n",
    "class IterableGcn(nn.Module):\n",
    "    \"\"\"\n",
    "    Sequential container for a list of GCN layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, layer_list):\n",
    "        super(IterableGcn, self).__init__()\n",
    "        self.layers = nn.ModuleList(layer_list)  # Store layers in ModuleList\n",
    "\n",
    "    def forward(self, x, edge_indices, edge_distances):\n",
    "        \"\"\"\n",
    "        Forward pass through all layers.\n",
    "        Args:\n",
    "            x: Node features.\n",
    "            edge_indices: Edge indices.\n",
    "            edge_distances: Edge weights.\n",
    "        Returns:\n",
    "            Output tensor after all layers.\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, edge_indices, edge_distances)  # Apply each layer\n",
    "        return x\n",
    "\n",
    "class GraphAutoencoder(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Hierarchical Graph Autoencoder with multiple pooling and unpooling stages.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, \n",
    "                 out_channels,\n",
    "                 n_units_input,\n",
    "                 n_units_output,\n",
    "                 n_layers_orig,\n",
    "                 n_units_orig,\n",
    "                 n_layers_pool_1,\n",
    "                 n_units_pool_1,\n",
    "                 n_layers_pool_2,\n",
    "                 n_units_pool_2\n",
    "                 ):\n",
    "        super(GraphAutoencoder, self).__init__()\n",
    "\n",
    "        self.n_units_input = n_units_input  # Number of units in input layer\n",
    "        self.n_units_output = n_units_output  # Number of units in output layer\n",
    "        self.n_layers_orig = n_layers_orig  # Number of original layers\n",
    "        self.n_units_orig = n_units_orig  # Units per original layer\n",
    "        self.n_layers_pool_1 = n_layers_pool_1  # Number of layers in first pooling\n",
    "        self.n_units_pool_1 = n_units_pool_1  # Units per first pooling layer\n",
    "        self.n_layers_pool_2 = n_layers_pool_2  # Number of layers in second pooling\n",
    "        self.n_units_pool_2 = n_units_pool_2  # Units per second pooling layer\n",
    "\n",
    "        # Encoder\n",
    "        self.graphconv_input = GraphConv(in_channels, n_units_input).to(device)  # Input GCN layer\n",
    "        self.graphconv_enc_orig = []  # List for encoder GCN layers (original space)\n",
    "        self.graphconv_dec_orig = []  # List for decoder GCN layers (original space)\n",
    "        for i in range(n_layers_orig):\n",
    "            if i == 0:\n",
    "                self.graphconv_enc_orig.append(GraphConv(n_units_input, n_units_orig[0]).to(device))  # First encoder layer\n",
    "                self.graphconv_dec_orig.append(GraphConv(n_units_orig[0], n_units_output).to(device))  # First decoder layer\n",
    "            else:\n",
    "                self.graphconv_enc_orig.append(GraphConv(n_units_orig[i-1], n_units_orig[i]).to(device))  # Subsequent encoder layers\n",
    "                self.graphconv_dec_orig.append(GraphConv(n_units_orig[i], n_units_orig[i-1]).to(device))  # Subsequent decoder layers\n",
    "\n",
    "        self.encode = Encoder().to(device)  # First encoder (sparse)\n",
    "        self.graphconv_pool_1 = []  # List for first pooling GCN layers\n",
    "        self.graphconv_unpool_1 = []  # List for first unpooling GCN layers\n",
    "        for i in range(n_layers_pool_1):\n",
    "            if i == 0:\n",
    "                self.graphconv_pool_1.append(GraphConv(n_units_orig[-1], n_units_pool_1[0]).to(device))  # First pooling layer\n",
    "                self.graphconv_unpool_1.append(GraphConv(n_units_pool_1[0], n_units_orig[-1]).to(device))  # First unpooling layer\n",
    "            else:\n",
    "                self.graphconv_pool_1.append(GraphConv(n_units_pool_1[i-1], n_units_pool_1[i]).to(device))  # Subsequent pooling layers\n",
    "                self.graphconv_unpool_1.append(GraphConv(n_units_pool_1[i], n_units_pool_1[i-1]).to(device))  # Subsequent unpooling layers\n",
    "\n",
    "        self.encode_1 = Encoder().to(device)  # Second encoder (sparse)\n",
    "        self.graphconv_pool_2 = []  # List for second pooling GCN layers\n",
    "        self.graphconv_unpool_2 = []  # List for second unpooling GCN layers\n",
    "        for i in range(n_layers_pool_2):\n",
    "            if i == 0:\n",
    "                self.graphconv_pool_2.append(GraphConv(n_units_pool_1[-1], n_units_pool_2[0]).to(device))  # First pooling layer (2nd stage)\n",
    "                self.graphconv_unpool_2.append(GraphConv(n_units_pool_2[0], n_units_pool_1[-1]).to(device))  # First unpooling layer (2nd stage)\n",
    "            else:\n",
    "                self.graphconv_pool_2.append(GraphConv(n_units_pool_2[i-1], n_units_pool_2[i]).to(device))  # Subsequent pooling layers (2nd stage)\n",
    "                self.graphconv_unpool_2.append(GraphConv(n_units_pool_2[i], n_units_pool_2[i-1]).to(device))  # Subsequent unpooling layers (2nd stage)\n",
    "\n",
    "        # Decoder\n",
    "        self.graphconv_unpool_2 = IterableGcn(list(reversed(self.graphconv_unpool_2))).to(device)  # Unpooling layers (2nd stage, reversed)\n",
    "        self.decode_1 = Decoder().to(device)  # Decoder (2nd stage)\n",
    "        self.graphconv_unpool_1 = IterableGcn(list(reversed(self.graphconv_unpool_1))).to(device)  # Unpooling layers (1st stage, reversed)\n",
    "        self.decode = Decoder().to(device)  # Decoder (1st stage)\n",
    "        self.graphconv_dec_orig = IterableGcn(list(reversed(self.graphconv_dec_orig))).to(device)  # Decoder layers (original space, reversed)\n",
    "\n",
    "        self.graphconv_enc_orig = IterableGcn(self.graphconv_enc_orig).to(device)  # Encoder layers (original space)\n",
    "        self.graphconv_pool_1 = IterableGcn(self.graphconv_pool_1).to(device)  # Pooling layers (1st stage)\n",
    "        self.graphconv_pool_2 = IterableGcn(self.graphconv_pool_2).to(device)  # Pooling layers (2nd stage)\n",
    "\n",
    "        self.out_conv_cp = GraphConv(n_units_output, out_channels, output=True).to(device)  # Output layer for cp\n",
    "        self.out_conv_cfx = GraphConv(n_units_output, out_channels, output=True).to(device)  # Output layer for cfx\n",
    "        self.out_conv_cfy = GraphConv(n_units_output, out_channels, output=True).to(device)  # Output layer for cfy\n",
    "        self.out_conv_cfz = GraphConv(n_units_output, out_channels, output=True).to(device)  # Output layer for cfz\n",
    "\n",
    "        self.set_requires_grad(False, [self.encode, self.encode_1, self.decode, self.decode_1])  # Freeze encoder/decoder\n",
    "\n",
    "    def set_requires_grad(self, requires_grad, modules):\n",
    "        \"\"\"\n",
    "        Set requires_grad for all parameters in the given modules.\n",
    "        Args:\n",
    "            requires_grad: Boolean flag.\n",
    "            modules: List of modules.\n",
    "        \"\"\"\n",
    "        for module in modules:\n",
    "            for param in module.parameters():\n",
    "                param.requires_grad = requires_grad  # Set requires_grad\n",
    "\n",
    "    def forward(self, x, space1: Space, space2: Space):\n",
    "        \"\"\"\n",
    "        Forward pass for the hierarchical graph autoencoder.\n",
    "        Args:\n",
    "            x: Input node features.\n",
    "            space1: Space object for first pooling/unpooling.\n",
    "            space2: Space object for second pooling/unpooling.\n",
    "        Returns:\n",
    "            Concatenated output tensor.\n",
    "        \"\"\"\n",
    "        x.to(device)  # Move input to device\n",
    "\n",
    "        # Input\n",
    "        x = self.graphconv_input(x, space1.edge_indices.to(device), space1.edge_distances.to(device))  # Input GCN\n",
    "\n",
    "        # Original space\n",
    "        x = self.graphconv_enc_orig(x, space1.edge_indices.to(device), space1.edge_distances.to(device))  # Encoder GCNs\n",
    "\n",
    "        # Encoder 1\n",
    "        x_enc = self.encode(x, space1.encoder_sparse_interpolation.to(device))  # Sparse encoding\n",
    "        x_enc = self.graphconv_pool_1(x_enc, space1.edge_indices_reduced.to(device), space1.edge_distances_reduced.to(device))  # Pooling GCNs\n",
    "\n",
    "        # Encoder 2\n",
    "        x_enc_1 = self.encode_1(x_enc, space2.encoder_sparse_interpolation.to(device))  # Sparse encoding (2nd stage)\n",
    "        x_enc_1 = self.graphconv_pool_2(x_enc_1, space2.edge_indices_reduced.to(device), space2.edge_distances_reduced.to(device))  # Pooling GCNs (2nd stage)\n",
    "\n",
    "        # Decoder 2\n",
    "        x_enc_1 = self.graphconv_unpool_2(x_enc_1, space2.edge_indices_reduced.to(device), space2.edge_distances_reduced.to(device))  # Unpooling GCNs (2nd stage)\n",
    "        x_dec_1 = self.decode_1(x_enc_1, space2.decoder_sparse_interpolation.to(device))  # Sparse decoding (2nd stage)\n",
    "\n",
    "        # Decoder 1\n",
    "        x_dec_1 = self.graphconv_unpool_1(x_dec_1, space1.edge_indices_reduced.to(device), space1.edge_distances_reduced.to(device))  # Unpooling GCNs (1st stage)\n",
    "        x_dec = self.decode(x_dec_1, space1.decoder_sparse_interpolation.to(device))  # Sparse decoding (1st stage)\n",
    "\n",
    "        # Original space\n",
    "        x_dec = self.graphconv_dec_orig(x_dec, space1.edge_indices.to(device), space1.edge_distances.to(device))  # Decoder GCNs (original space)\n",
    "\n",
    "        # Output\n",
    "        x_cp = self.out_conv_cp(x_dec, space1.edge_indices.to(device), space1.edge_distances.to(device))  # Output GCN cp\n",
    "        x_cfx = self.out_conv_cfx(x_dec, space1.edge_indices.to(device), space1.edge_distances.to(device))  # Output GCN cfx\n",
    "        x_cfy = self.out_conv_cfy(x_dec, space1.edge_indices.to(device), space1.edge_distances.to(device))  # Output GCN cfy\n",
    "        x_cfz = self.out_conv_cfz(x_dec, space1.edge_indices.to(device), space1.edge_distances.to(device))  # Output GCN cfz\n",
    "\n",
    "        x_out = torch.cat((x_cp, x_cfx, x_cfy, x_cfz), dim=1)  # Concatenate outputs\n",
    "\n",
    "        return x_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Aero Coeff for Loss\n",
    "\n",
    "X_min = torch.tensor(scaler.data_min_, dtype=torch.float32).to(device)  # Minimum values for X features\n",
    "X_max = torch.tensor(scaler.data_max_, dtype=torch.float32).to(device)  # Maximum values for X features\n",
    "Y_min = torch.tensor(scalery.data_min_, dtype=torch.float32).to(device)  # Minimum values for Y features\n",
    "Y_max = torch.tensor(scalery.data_max_, dtype=torch.float32).to(device)  # Maximum values for Y features\n",
    "\n",
    "def compute_aero_coeff(ds, AoA, X_Grid_K_Unit_Normal, Y_Grid_K_Unit_Normal, Z_Grid_K_Unit_Normal, cell_area, coordinates_tensor):\n",
    "    \"\"\"\n",
    "    Compute aerodynamic moment coefficient for a given dataset.\n",
    "\n",
    "    Args:\n",
    "        ds (Tensor): Input tensor containing aerodynamic data.\n",
    "        AoA (float): Angle of attack in degrees.\n",
    "        X_Grid_K_Unit_Normal (Tensor): X component of grid normal.\n",
    "        Y_Grid_K_Unit_Normal (Tensor): Y component of grid normal.\n",
    "        Z_Grid_K_Unit_Normal (Tensor): Z component of grid normal.\n",
    "        cell_area (Tensor): Area of each cell.\n",
    "        coordinates_tensor (Tensor): Coordinates of each node.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: Computed aerodynamic moment.\n",
    "    \"\"\"\n",
    "    aoa = AoA * np.pi / 180.0  # Convert AoA to radians\n",
    "\n",
    "    shear_x = ds[:, 1]  # Extract shear force in x-direction\n",
    "    shear_z = ds[:, 3]  # Extract shear force in z-direction\n",
    "    cp = ds[:, 0]       # Extract pressure coefficient\n",
    "\n",
    "    taux = (shear_x - X_Grid_K_Unit_Normal * cp) / area  # Compute x-direction shear stress\n",
    "    tauz = (shear_z - Z_Grid_K_Unit_Normal * cp) / area  # Compute z-direction shear stress\n",
    "\n",
    "    my = (coordinates_tensor[:, 2] * (taux * torch.cos(aoa) + tauz * torch.sin(aoa)) - \n",
    "          (coordinates_tensor[:, 0] - xref) * (tauz * torch.cos(aoa) - taux * torch.sin(aoa))) / chord  # Compute moment arm\n",
    "\n",
    "    moment = torch.sum(my * cell_area)  # Sum moments over all cells\n",
    "\n",
    "    return moment\n",
    "\n",
    "def integral_load(x, y, out, X_Grid_K_Unit_Normal, Y_Grid_K_Unit_Normal, Z_Grid_K_Unit_Normal, cell_area, coordinates_tensor):\n",
    "    \"\"\"\n",
    "    Compute the mean squared error between predicted and true aerodynamic moments.\n",
    "\n",
    "    Args:\n",
    "        x (Tensor): Input features (denormalized).\n",
    "        y (Tensor): True output values (denormalized).\n",
    "        out (Tensor): Predicted output values (denormalized).\n",
    "        X_Grid_K_Unit_Normal (Tensor): X component of grid normal.\n",
    "        Y_Grid_K_Unit_Normal (Tensor): Y component of grid normal.\n",
    "        Z_Grid_K_Unit_Normal (Tensor): Z component of grid normal.\n",
    "        cell_area (Tensor): Area of each cell.\n",
    "        coordinates_tensor (Tensor): Coordinates of each node.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: Mean squared error of the aerodynamic moment.\n",
    "    \"\"\"\n",
    "    global X_min, X_max, Y_min, Y_max\n",
    "\n",
    "    x_den = x - X_min / (X_max - X_min)  # Denormalize input features\n",
    "    out_den = out - Y_min / (Y_max - Y_min)  # Denormalize predicted outputs\n",
    "    y_den = y - Y_min / (Y_max - Y_min)  # Denormalize true outputs\n",
    "\n",
    "    AoA = torch.tensor(0.0, dtype=torch.float32)  # Set angle of attack to zero\n",
    "\n",
    "    moment_NN = compute_aero_coeff(out_den, AoA, X_Grid_K_Unit_Normal, Y_Grid_K_Unit_Normal, Z_Grid_K_Unit_Normal, cell_area, coordinates_tensor)  # Predicted moment\n",
    "    moment_CFD = compute_aero_coeff(y_den, AoA, X_Grid_K_Unit_Normal, Y_Grid_K_Unit_Normal, Z_Grid_K_Unit_Normal, cell_area, coordinates_tensor)  # True moment\n",
    "\n",
    "    squared_error = (moment_CFD - moment_NN) ** 2  # Compute squared error\n",
    "    error_moment = torch.mean(squared_error)  # Compute mean squared error\n",
    "\n",
    "    return error_moment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Objective function for Optuna hyperparameter optimization.\n",
    "    Defines the model architecture, training loop, validation, and early stopping.\n",
    "    Returns the average validation loss for the current trial.\n",
    "    \"\"\"\n",
    "    layers_orig = trial.suggest_int('layers_orig', 1, 3, step=1)  # Number of original layers\n",
    "    layers_pool_1 = trial.suggest_int('layers_pool_1', 1, 3, step=1)  # Number of pool_1 layers\n",
    "    layers_pool_2 = trial.suggest_int('layers_pool_2', 1, 3, step=1)  # Number of pool_2 layers\n",
    "\n",
    "    n_units_input = trial.suggest_int('n_units_input', 32, 256, step=16)  # Input units\n",
    "    n_units_output = trial.suggest_int('n_units_output', 32, 256, step=16)  # Output units\n",
    "    units_orig = [trial.suggest_int(f'units_orig_{str(i+1)}', 32, 256, step=16) for i in range(int(layers_orig))]  # Units per orig layer\n",
    "    units_pool_1 = [trial.suggest_int(f'units_pool_1_{str(i+1)}', 32, 384, step=16) for i in range(int(layers_pool_1))]  # Units per pool_1 layer\n",
    "    units_pool_2 = [trial.suggest_int(f'units_pool_2_{str(i+1)}', 32, 512, step=16) for i in range(int(layers_pool_2))]  # Units per pool_2 layer\n",
    "\n",
    "    model = GraphAutoencoder(\n",
    "        in_channels=X_train.shape[-1],  # Number of input channels\n",
    "        out_channels=1,  # Number of output channels\n",
    "        n_units_input=n_units_input,  # Input units\n",
    "        n_units_output=n_units_output,  # Output units\n",
    "        n_layers_orig=layers_orig,  # Original layers\n",
    "        n_units_orig=units_orig,  # Units per orig layer\n",
    "        n_layers_pool_1=layers_pool_1,  # Pool_1 layers\n",
    "        n_units_pool_1=units_pool_1,  # Units per pool_1 layer\n",
    "        n_layers_pool_2=layers_pool_2,  # Pool_2 layers\n",
    "        n_units_pool_2=units_pool_2  # Units per pool_2 layer\n",
    "    ).to(device)  # Move model to device\n",
    "\n",
    "    n_epochs = 500  # Number of epochs\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer\n",
    "    mae_loss = nn.L1Loss()  # Mean Absolute Error loss\n",
    "\n",
    "    scheduler = StepLR(optimizer, step_size=30, gamma=0.9)  # Learning rate scheduler\n",
    "\n",
    "    early_stopping_patience = 100  # Early stopping patience\n",
    "    best_val_loss = float(\"inf\")  # Best validation loss\n",
    "    epochs_without_improvement = 0  # Counter for early stopping\n",
    "    lambda_factor = 0.01  # Weight for error moment\n",
    "\n",
    "    for epoch in range(n_epochs):  # Training loop\n",
    "        model.train()  # Set model to training mode\n",
    "        total_loss = 0  # Initialize total loss\n",
    "        for data in train_loader:  # Iterate over training data\n",
    "            optimizer.zero_grad()  # Zero gradients\n",
    "            for i in range(len(data)):  # Iterate over batch\n",
    "                x, y, space, grid_data = data[i].x, data[i].y, data[i].space, data[i].grid_data  # Unpack data\n",
    "                cell_area, X_Grid_K_Unit_Normal, Y_Grid_K_Unit_Normal, Z_Grid_K_Unit_Normal, coordinates_tensor = grid_data  # Unpack grid data\n",
    "                out = model(x, space[0], space[1])  # Forward pass\n",
    "                loss = mae_loss(out, y)  # Compute MAE loss\n",
    "                error_moment = integral_load(x, y, out, X_Grid_K_Unit_Normal, Y_Grid_K_Unit_Normal, Z_Grid_K_Unit_Normal, cell_area, coordinates_tensor)  # Compute error moment\n",
    "                combined_loss = loss + lambda_factor * error_moment  # Combine losses\n",
    "                combined_loss.backward()  # Backpropagation\n",
    "                optimizer.step()  # Optimizer step\n",
    "                total_loss += combined_loss.item()  # Accumulate loss\n",
    "\n",
    "        average_train_loss = total_loss / len(train_loader)  # Average training loss\n",
    "\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        total_val_loss = 0  # Initialize validation loss\n",
    "        with torch.no_grad():  # Disable gradient computation\n",
    "            for data in val_loader:  # Iterate over validation data\n",
    "                for i in range(len(data)):  # Iterate over batch\n",
    "                    x, y, space, grid_data = data[i].x, data[i].y, data[i].space, data[i].grid_data  # Unpack data\n",
    "                    cell_area, X_Grid_K_Unit_Normal, Y_Grid_K_Unit_Normal, Z_Grid_K_Unit_Normal, coordinates_tensor = grid_data  # Unpack grid data\n",
    "                    out = model(x, space[0], space[1])  # Forward pass\n",
    "                    val_loss = mae_loss(out, y)  # Compute validation loss\n",
    "                    error_moment_val = integral_load(x, y, out, X_Grid_K_Unit_Normal, Y_Grid_K_Unit_Normal, Z_Grid_K_Unit_Normal, cell_area, coordinates_tensor)  # Compute error moment\n",
    "                    combined_loss_val = val_loss + lambda_factor * error_moment_val  # Combine losses\n",
    "                    total_val_loss += combined_loss_val.item()  # Accumulate validation loss\n",
    "\n",
    "        average_val_loss = total_val_loss / len(val_loader)  # Average validation loss\n",
    "\n",
    "        current_lr = optimizer.param_groups[0]['lr']  # Get current learning rate\n",
    "        if current_lr > 0.00005:  # Check learning rate threshold\n",
    "            scheduler.step()  # Step scheduler\n",
    "\n",
    "        if average_val_loss < best_val_loss:  # Check for improvement\n",
    "            best_val_loss = average_val_loss  # Update best validation loss\n",
    "            epochs_without_improvement = 0  # Reset counter\n",
    "            # Model saving code can be added here if needed\n",
    "        else:\n",
    "            epochs_without_improvement += 1  # Increment counter\n",
    "\n",
    "        if epochs_without_improvement >= early_stopping_patience:  # Early stopping condition\n",
    "            break  # Stop training\n",
    "\n",
    "        if 'lowest_loss' not in trial.user_attrs or average_val_loss < trial.user_attrs['lowest_loss']:  # Track lowest loss\n",
    "            trial.set_user_attr('lowest_loss', average_val_loss)  # Set lowest loss\n",
    "\n",
    "    return average_val_loss  # Return validation loss\n",
    "\n",
    "trial_values = []  # List to store the loss values at each optimizer trial\n",
    "\n",
    "if Execute_Train_flag:\n",
    "    # Create a study object with TPE sampler (Bayesian optimization) and optimize hyperparameters\n",
    "    study = optuna.create_study(\n",
    "        study_name=study_name,  # Name of the study\n",
    "        storage=storage_url,  # Storage URL\n",
    "        direction='minimize',  # Minimize the objective\n",
    "        sampler=optuna.samplers.TPESampler(seed=seed_value)  # TPE sampler with seed\n",
    "    )\n",
    "    study.optimize(objective, n_trials=20)  # Run optimization\n",
    "\n",
    "    for trial in study.trials:  # Extract the lowest loss value for each trial\n",
    "        trial_values.append(trial.user_attrs['lowest_loss'])  # Append loss\n",
    "\n",
    "if Execute_Train_flag:\n",
    "    # Store the loss value at each trial in a file\n",
    "    with open(\"val_loss_vs_optimizer_trials.txt\", \"w\") as f:  # Open file for writing\n",
    "        for trial_number, loss_value in enumerate(trial_values, start=1):  # Enumerate trial values\n",
    "            f.write(f\"Trial {trial_number}: {loss_value}\\n\")  # Write to file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Transfer_Learning_flag:\n",
    "    study = optuna.load_study(study_name=study_name, storage=storage_url)\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "layers_orig = best_params['layers_orig']\n",
    "layers_pool_1 = best_params['layers_pool_1']\n",
    "layers_pool_2 = best_params['layers_pool_2']\n",
    "n_units_input = best_params['n_units_input']\n",
    "n_units_output = best_params['n_units_output']\n",
    "units_orig =   [best_params[f'units_orig_{str(i+1)}'] for i in range(int(layers_orig))]\n",
    "units_pool_1 = [best_params[f'units_pool_1_{str(i+1)}'] for i in range(int(layers_pool_1))]\n",
    "units_pool_2 = [best_params[f'units_pool_2_{str(i+1)}'] for i in range(int(layers_pool_2))]\n",
    "        \n",
    "model = GraphAutoencoder(in_channels=X_train.shape[-1], \n",
    "                out_channels=1,\n",
    "                n_units_input = n_units_input,\n",
    "                n_units_output = n_units_output,\n",
    "                n_layers_orig = layers_orig,\n",
    "                n_units_orig = units_orig,\n",
    "                n_layers_pool_1 = layers_pool_1,\n",
    "                n_units_pool_1 = units_pool_1,\n",
    "                n_layers_pool_2 = layers_pool_2,\n",
    "                n_units_pool_2 = units_pool_2\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    \"\"\"\n",
    "    Counts the number of trainable parameters in a PyTorch model.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model to count parameters for.\n",
    "\n",
    "    Returns:\n",
    "        int: Number of trainable parameters.\n",
    "    \"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)  # Sum parameters that require gradients\n",
    "\n",
    "print('\\n')\n",
    "print('Compiling GCN model')\n",
    "\n",
    "num_params = count_parameters(model)  # Count trainable parameters in the model\n",
    "print(f\"Number of trainable parameters in the model: {num_params} \\n\")\n",
    "\n",
    "train_losses = []  # List to store training losses per epoch\n",
    "val_losses = []    # List to store validation losses per epoch\n",
    "\n",
    "# print(model)\n",
    "\n",
    "if Execute_Train_flag:  # Check if training should be executed\n",
    "\n",
    "    n_epochs = 2000  # Number of training epochs\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer\n",
    "    mae_loss = nn.L1Loss()  # Mean Absolute Error loss\n",
    "\n",
    "    scheduler = StepLR(optimizer, step_size=30, gamma=0.9)  # Learning rate scheduler\n",
    "\n",
    "    early_stopping_patience = 200  # Early stopping patience\n",
    "    best_val_loss = float(\"inf\")  # Initialize best validation loss\n",
    "    epochs_without_improvement = 0  # Counter for early stopping\n",
    "    lambda_factor = 0.01  # Weight for error moment in loss\n",
    "\n",
    "    for epoch in range(n_epochs):  # Loop over epochs\n",
    "        model.train()  # Set model to training mode\n",
    "        total_loss = 0  # Accumulate training loss\n",
    "\n",
    "        for data in train_loader:  # Iterate over training batches\n",
    "            optimizer.zero_grad()  # Zero gradients\n",
    "\n",
    "            for i in range(len(data)):  # Iterate over items in batch\n",
    "                x, y, space, grid_data = data[i].x, data[i].y, data[i].space, data[i].grid_data  # Unpack data\n",
    "                cell_area, X_Grid_K_Unit_Normal, Y_Grid_K_Unit_Normal, Z_Grid_K_Unit_Normal, coordinates_tensor = grid_data  # Unpack grid data\n",
    "                out = model(x, space[0], space[1])  # Forward pass\n",
    "                loss = mae_loss(out, y)  # Compute MAE loss\n",
    "                error_moment = integral_load(x, y, out, X_Grid_K_Unit_Normal, Y_Grid_K_Unit_Normal, Z_Grid_K_Unit_Normal, cell_area, coordinates_tensor)  # Compute error moment\n",
    "                combined_loss = loss + lambda_factor * error_moment  # Combine losses\n",
    "                combined_loss.backward()  # Backpropagation\n",
    "                optimizer.step()  # Update parameters\n",
    "                total_loss += combined_loss.item()  # Accumulate loss\n",
    "\n",
    "        average_train_loss = total_loss / len(train_loader)  # Average training loss\n",
    "\n",
    "        # Validation\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        total_val_loss = 0  # Accumulate validation loss\n",
    "        with torch.no_grad():  # Disable gradient computation\n",
    "            for data in val_loader:  # Iterate over validation batches\n",
    "                for i in range(len(data)):  # Iterate over items in batch\n",
    "                    x, y, space, grid_data = data[i].x, data[i].y, data[i].space, data[i].grid_data  # Unpack data\n",
    "                    cell_area, X_Grid_K_Unit_Normal, Y_Grid_K_Unit_Normal, Z_Grid_K_Unit_Normal, coordinates_tensor = grid_data  # Unpack grid data\n",
    "                    out = model(x, space[0], space[1])  # Forward pass\n",
    "                    val_loss = mae_loss(out, y)  # Compute MAE loss\n",
    "                    error_moment_val = integral_load(x, y, out, X_Grid_K_Unit_Normal, Y_Grid_K_Unit_Normal, Z_Grid_K_Unit_Normal, cell_area, coordinates_tensor)  # Compute error moment\n",
    "                    combined_loss_val = val_loss + lambda_factor * error_moment_val  # Combine losses\n",
    "                    total_val_loss += combined_loss_val.item()  # Accumulate loss\n",
    "\n",
    "        average_val_loss = total_val_loss / len(val_loader)  # Average validation loss\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, Train Loss: {average_train_loss}, Val Loss: {average_val_loss}\")  # Print losses\n",
    "\n",
    "        train_losses.append(average_train_loss)  # Store training loss\n",
    "        val_losses.append(average_val_loss)  # Store validation loss\n",
    "\n",
    "        current_lr = optimizer.param_groups[0]['lr']  # Get current learning rate\n",
    "        if current_lr > 0.00005:\n",
    "            scheduler.step()  # Update learning rate\n",
    "\n",
    "        # Early stopping check\n",
    "        if average_val_loss < best_val_loss:\n",
    "            best_val_loss = average_val_loss  # Update best validation loss\n",
    "            epochs_without_improvement = 0  # Reset counter\n",
    "            model_save_path = os.path.join(model_dir, model_filename)  # Path to save model\n",
    "\n",
    "            if os.path.exists(model_save_path):\n",
    "                os.remove(model_save_path)  # Remove existing model file\n",
    "\n",
    "            torch.save(model.state_dict(), model_save_path)  # Save model state\n",
    "        else:\n",
    "            epochs_without_improvement += 1  # Increment counter\n",
    "\n",
    "        # Check for early stopping\n",
    "        if epochs_without_improvement >= early_stopping_patience:\n",
    "            print(f\"Early stopping at {epoch + 1} after {early_stopping_patience} epochs without improvement.\")  # Early stopping message\n",
    "            break  # Stop training\n",
    "\n",
    "    # Plot the training and validation loss over epochs\n",
    "    plt.figure(figsize=(10, 6))  # Create figure\n",
    "    plt.plot(range(1, len(train_losses) + 1), train_losses, label='Train Loss')  # Plot training loss\n",
    "    plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')  # Plot validation loss\n",
    "    plt.xlabel('Epoch')  # X-axis label\n",
    "    plt.ylabel('Loss (MAE)')  # Y-axis label\n",
    "    plt.ylim(0, 0.1)  # Set y-axis limits\n",
    "    plt.legend()  # Show legend\n",
    "    plt.title('Training and Validation Loss vs Epochs')  # Plot title\n",
    "    plt.grid(True)  # Show grid\n",
    "    plt.show()  # Display plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_and_inverse_transform(model, test_loader, Y_test, X_test, scaler, scalery):\n",
    "    \"\"\"\n",
    "    Test the model on the test set and inverse transform the predictions, X_test, and Y_test.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): Trained model to evaluate.\n",
    "        test_loader (DataLoader): DataLoader for the test set.\n",
    "        Y_test (np.ndarray): Ground truth outputs for the test set.\n",
    "        X_test (np.ndarray): Input features for the test set.\n",
    "        scaler (MinMaxScaler): Scaler used for X normalization.\n",
    "        scalery (MinMaxScaler): Scaler used for Y normalization.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (predictions, X_test_inv, Y_test_inv) - inverse transformed predictions, X_test, and Y_test.\n",
    "    \"\"\"\n",
    "    predictions = []  # List to store model predictions\n",
    "    with torch.no_grad():  # Disable gradient computation for inference\n",
    "        for data in test_loader:  # Iterate over test batches\n",
    "            for i in range(len(data)):  # Iterate over items in batch\n",
    "                x, space = data[i].x, data[i].space  # Get input and space\n",
    "                out = model(x, space[0], space[1])  # Forward pass\n",
    "                predictions.append(out)  # Store prediction\n",
    "\n",
    "    predictions = torch.cat(predictions, dim=0)  # Concatenate predictions\n",
    "\n",
    "    samples, points, variables = Y_test.shape  # Get shape of Y_test\n",
    "    predictions = np.array(predictions.cpu().numpy())  # Convert predictions to numpy array\n",
    "    predictions = predictions.reshape(samples, points, variables)  # Reshape predictions\n",
    "\n",
    "    samples, points, variables = X_test.shape  # Get shape of X_test\n",
    "    X_test_reshaped = np.reshape(X_test, newshape=(-1, variables))  # Flatten X_test\n",
    "    X_test_inv = scaler.inverse_transform(X_test_reshaped)  # Inverse transform X_test\n",
    "    X_test_inv = np.reshape(X_test_inv, newshape=(samples, points, variables))  # Reshape X_test\n",
    "\n",
    "    samples, points, variables = predictions.shape  # Get shape of predictions\n",
    "    predictions_reshaped = np.reshape(predictions, newshape=(-1, variables))  # Flatten predictions\n",
    "    predictions_inv = scalery.inverse_transform(predictions_reshaped)  # Inverse transform predictions\n",
    "    predictions_inv = np.reshape(predictions_inv, newshape=(samples, points, variables))  # Reshape predictions\n",
    "\n",
    "    samples, points, variables = Y_test.shape  # Get shape of Y_test\n",
    "    Y_test_reshaped = np.reshape(Y_test, newshape=(-1, variables))  # Flatten Y_test\n",
    "    Y_test_inv = scalery.inverse_transform(Y_test_reshaped)  # Inverse transform Y_test\n",
    "    Y_test_inv = np.reshape(Y_test_inv, newshape=(samples, points, variables))  # Reshape Y_test\n",
    "\n",
    "    return predictions_inv, X_test_inv, Y_test_inv  # Return denormalized arrays\n",
    "\n",
    "if Transfer_Learning_flag:\n",
    "    # Load the best model if not trained in this session\n",
    "    if not Execute_Train_flag:\n",
    "        model.load_state_dict(torch.load(model_dir + model_filename))  # Load model weights\n",
    "\n",
    "    predictions_test, X_test_inv, Y_test_inv = test_model_and_inverse_transform(\n",
    "        model=model,  # Trained model\n",
    "        test_loader=test_loader,  # Test DataLoader\n",
    "        Y_test=Y_test,  # Ground truth outputs\n",
    "        X_test=X_test,  # Test inputs\n",
    "        scaler=scaler,  # X scaler\n",
    "        scalery=scalery  # Y scaler\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
